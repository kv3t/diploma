import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.preprocessing import MinMaxScaler
from sklearn.model_selection import ParameterGrid, ParameterSampler
from scikeras.wrappers import KerasRegressor  
from skopt import BayesSearchCV
from skopt.space import Real, Integer, Categorical
import tensorflow as tf
from sklearn.metrics import mean_absolute_error, root_mean_squared_error
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Dropout, Input
from tensorflow.keras.optimizers import Adam
from deap import base, creator, tools, algorithms
import random

def load_data(filepath):
    data = pd.read_csv(filepath)
    data['date'] = pd.to_datetime(data['date'])
    data.set_index('date', inplace=True)
    return data

def prepare_data(data, look_back=60):
    scaler = MinMaxScaler(feature_range=(0, 1))
    scaled_data = scaler.fit_transform(data[['close', 'volume']].values)

    X, y = [], []
    for i in range(look_back, len(scaled_data)):
        X.append(scaled_data[i-look_back:i, :])
        y.append(scaled_data[i, 0])  
    return np.array(X), np.array(y), scaler

def build_lstm_model(nodes, hidden_layers, dropout_rate, learning_rate, batch_size, epochs, input_shape):
    model = Sequential()
    model.add(Input(shape=input_shape))
    model.add(LSTM(units=nodes, return_sequences=True))
    model.add(Dropout(dropout_rate))
    for _ in range(int(hidden_layers) - 1):
        model.add(LSTM(units=nodes, return_sequences=True))
        model.add(Dropout(dropout_rate))
    model.add(LSTM(units=nodes))
    model.add(Dropout(dropout_rate))
    model.add(Dense(units=1))
    optimizer = Adam(learning_rate=learning_rate)
    model.compile(optimizer=optimizer, loss='mean_absolute_error')
    return model

def build_simple_lstm_model(input_shape, nodes=100, dropout_rate=0.2, learning_rate=0.001):
    model = Sequential()
    model.add(Input(shape=input_shape))
    model.add(LSTM(units=nodes, return_sequences=True))
    model.add(Dropout(dropout_rate))
    model.add(LSTM(units=nodes))
    model.add(Dropout(dropout_rate))
    model.add(Dense(units=1))
    optimizer = Adam(learning_rate=learning_rate)
    model.compile(optimizer=optimizer, loss='mean_absolute_error')
    return model

def train_and_evaluate_model(model, X_train, y_train, X_test, y_test, scaler, epochs=50, batch_size=32, validation_split=0.2):
    history = model.fit(
        X_train, y_train,
        epochs=epochs,
        batch_size=batch_size,
        validation_split=validation_split,
        verbose=1
    )
    
    y_pred_train = model.predict(X_train)
    y_pred_test = model.predict(X_test)
    
    y_pred_train = scaler.inverse_transform(np.concatenate((y_pred_train, np.zeros((len(y_pred_train), 1))), axis=1))[:, 0]
    y_pred_test = scaler.inverse_transform(np.concatenate((y_pred_test, np.zeros((len(y_pred_test), 1))), axis=1))[:, 0]
    
    y_train_actual = scaler.inverse_transform(np.concatenate((y_train.reshape(-1, 1), np.zeros((len(y_train), 1))), axis=1))[:, 0]
    y_test_actual = scaler.inverse_transform(np.concatenate((y_test.reshape(-1, 1), np.zeros((len(y_test), 1))), axis=1))[:, 0]
    
    test_mae = mean_absolute_error(y_pred_test, y_test_actual)
    test_rmse = root_mean_squared_error(y_pred_test, y_test_actual)
    
    return model, y_pred_train, y_pred_test, test_rmse, test_mae, history

def grid_search_simple_model(X_train, y_train, X_test, y_test, scaler):
    param_grid = {
        'nodes': [50, 100, 150],
        'dropout_rate': [0.1, 0.2, 0.3],
        'learning_rate': [0.001, 0.01],
        'batch_size': [32, 64],
        'hidden_layers': [1, 2],
        'epochs': [25, 50, 75, 100]
    }
    
    best_score = float('inf')
    best_params = None
    
    print(f"\nStarting Grid Search with {len(ParameterGrid(param_grid))} combinations...")
    for params in ParameterGrid(param_grid):
        model = build_lstm_model(
            input_shape=(X_train.shape[1], X_train.shape[2]),
            nodes=params['nodes'],
            dropout_rate=params['dropout_rate'],
            learning_rate=params['learning_rate'],
            hidden_layers=params['hidden_layers'],
            epochs=params['epochs'], 
            batch_size=params['batch_size']
        )
        
        _, _, _, _, test_rmse, _ = train_and_evaluate_model(
            model, X_train, y_train, X_test, y_test, scaler,
            epochs=params['epochs'], batch_size=params['batch_size'], validation_split=0.2
        )
        
        if test_rmse < best_score:
            best_score = test_rmse
            best_params = params
    
    print("\nGrid Search completed!")
    print(f"Best params: {best_params}")
    print(f"Best RMSE: {best_score:.4f}")
    
    model = build_lstm_model(
        input_shape=(X_train.shape[1], X_train.shape[2]),
        **best_params
    )
    return train_and_evaluate_model(
        model, X_train, y_train, X_test, y_test, scaler,
        epochs=best_params['epochs'], batch_size=best_params['batch_size'], validation_split=0.2
    )

def random_search_simple_model(X_train, y_train, X_test, y_test, scaler, n_iter=10, epochs=50):
    param_dist = {
        'nodes': [50, 100, 150, 200],
        'dropout_rate': [0.1, 0.2, 0.3, 0.4],
        'learning_rate': [0.0001, 0.001, 0.01],
        'batch_size': [32, 64, 128],
        'hidden_layers': [1, 2],
        'epochs': [25, 50, 75, 100]
    }
    
    best_score = float('inf')
    best_params = None
    
    print(f"\nStarting Random Search with {n_iter} iterations...")
    for params in ParameterSampler(param_dist, n_iter=n_iter):
        model = build_lstm_model(
            input_shape=(X_train.shape[1], X_train.shape[2]),
            nodes=params['nodes'],
            dropout_rate=params['dropout_rate'],
            learning_rate=params['learning_rate'],
            hidden_layers=params['hidden_layers'],
            epochs=params['epochs'], 
            batch_size=params['batch_size']
        )
        
        _, _, _, _, test_rmse, _ = train_and_evaluate_model(
            model, X_train, y_train, X_test, y_test, scaler,
            epochs=params['epochs'], batch_size=params['batch_size'], validation_split=0.2
        )
        
        if test_rmse < best_score:
            best_score = test_rmse
            best_params = params
    
    print("\nRandom Search completed!")
    print(f"Best params: {best_params}")
    print(f"Best RMSE: {best_score:.4f}")
    
    model = build_lstm_model(
        input_shape=(X_train.shape[1], X_train.shape[2]),
        **best_params
    )
    return train_and_evaluate_model(
        model, X_train, y_train, X_test, y_test, scaler,
        epochs=best_params['epochs'], batch_size=best_params['batch_size'], validation_split=0.2
    )

def bayesian_optimization_simple_model(X_train, y_train, X_test, y_test, scaler, n_iter=10, epochs=50):
    search_spaces = {
        'model__nodes': Integer(50, 200),
        'model__dropout_rate': Real(0.1, 0.5),
        'model__learning_rate': Real(0.0001, 0.01, 'log-uniform'),
        'batch_size': Categorical([32, 64, 128]),
        'model__hidden_layers': Integer(1, 2),
        'epochs': Integer(25, 100),
    }
    
    def create_model(nodes=100, dropout_rate=0.2, learning_rate=0.001, hidden_layers = 1, epochs = 50, batch_size = 32):
        model = Sequential()
        model.add(Input(shape=(X_train.shape[1], X_train.shape[2])))
        model.add(LSTM(units=nodes, return_sequences=True))
        model.add(Dropout(dropout_rate))
        model.add(LSTM(units=nodes))
        model.add(Dropout(dropout_rate))
        model.add(Dense(units=1))
        optimizer = Adam(learning_rate=learning_rate)
        model.compile(optimizer=optimizer, loss='mean_absolute_error')
        return model
    
    model = KerasRegressor(
        model=create_model,
        epochs=epochs,
        verbose=0
    )
    
    print(f"\nStarting Bayesian Optimization with {n_iter} iterations...")
    bayes_search = BayesSearchCV(
        estimator=model,
        search_spaces=search_spaces,
        n_iter=n_iter,
        cv=3,
        verbose=1,
        n_jobs=1,
        scoring='neg_mean_absolute_error'
    )
    
    bayes_search.fit(X_train, y_train)
    best_params = bayes_search.best_params_
    
    print("\nBayesian Optimization completed!")
    print(f"Best params: {best_params}")
    print(f"Best score: {-bayes_search.best_score_:.4f}")
    
    model = build_lstm_model(
        input_shape=(X_train.shape[1], X_train.shape[2]),
        nodes=best_params['model__nodes'],
        dropout_rate=best_params['model__dropout_rate'],
        learning_rate=best_params['model__learning_rate'],
        hidden_layers=best_params['model__hidden_layers'],
        epochs=best_params['epochs'], 
        batch_size=best_params['batch_size']
    )
    return train_and_evaluate_model(
        model, X_train, y_train, X_test, y_test, scaler,
        epochs=best_params['epochs'], batch_size=best_params['batch_size'], validation_split=0.2
    )


def setup_genetic_algorithm():
    creator.create("FitnessMin", base.Fitness, weights=(-1.0,))
    creator.create("Individual", list, fitness=creator.FitnessMin)

    toolbox = base.Toolbox()
    toolbox.register("attr_nodes", random.randint, 50, 200)
    toolbox.register("attr_hidden_layers", random.randint, 1, 3)
    toolbox.register("attr_dropout_rate", random.uniform, 0.0, 0.5)
    toolbox.register("attr_batch_size", random.randint, 32, 128)
    toolbox.register("attr_epochs", random.randint, 10, 100)
    toolbox.register("attr_learning_rate", random.uniform, 0.0001, 0.01)

    toolbox.register("individual", tools.initCycle, creator.Individual,
                    (toolbox.attr_nodes, toolbox.attr_hidden_layers, toolbox.attr_dropout_rate,
                     toolbox.attr_batch_size, toolbox.attr_epochs, toolbox.attr_learning_rate), n=1)
    toolbox.register("population", tools.initRepeat, list, toolbox.individual)
    toolbox.register("mate", tools.cxTwoPoint)
    toolbox.register("mutate", custom_mutate, indpb=0.2)
    toolbox.register("select", tools.selTournament, tournsize=3)
    
    return toolbox

def custom_mutate(individual, indpb):
    for i in range(len(individual)):
        if random.random() < indpb:
            if i == 0:  # nodes
                individual[i] = random.randint(50, 200)
            elif i == 1:  # hidden_layers
                individual[i] = random.randint(1, 3)
            elif i == 2:  # dropout_rate
                individual[i] = random.uniform(0.0, 0.5)
            else:
                individual[i] += random.gauss(0, 1)
                if i == 3:  # batch_size
                    individual[i] = max(32, min(128, individual[i]))
                elif i == 4:  # epochs
                    individual[i] = max(10, min(100, individual[i]))
                elif i == 5:  # learning_rate
                    individual[i] = max(0.0001, min(0.01, individual[i]))
    return individual,

def genetic_algorithm_optimization(X_train, y_train, X_test, y_test, scaler):
    def evaluate(individual):
        nodes, hidden_layers, dropout_rate, batch_size, epochs, learning_rate = individual
        nodes = int(nodes)
        hidden_layers = int(hidden_layers)
        dropout_rate = max(0.0, min(0.5, dropout_rate))
        epochs = int(epochs)
        batch_size = int(batch_size)
        
        model = build_lstm_model(
            nodes, hidden_layers, dropout_rate, learning_rate, batch_size, epochs,
            (X_train.shape[1], X_train.shape[2])
        )
        
        model.fit(
            X_train, y_train,
            epochs=epochs,
            batch_size=batch_size,
            verbose=0,
            validation_split=0.2
        )
        
        y_pred = model.predict(X_test)
        y_pred = scaler.inverse_transform(np.concatenate((y_pred, np.zeros((len(y_pred), 1))), axis=1))[:, 0]
        y_test_actual = scaler.inverse_transform(np.concatenate((y_test.reshape(-1, 1), np.zeros((len(y_test), 1))), axis=1))[:, 0]
        return np.sqrt(np.mean((y_pred - y_test_actual)**2)), 
    
    toolbox = setup_genetic_algorithm()
    toolbox.register("evaluate", evaluate)
    
    population_size = 50
    n_generations = 10
    cxpb, mutpb = 0.5, 0.2
    
    population = toolbox.population(n=population_size)
    hof = tools.HallOfFame(1)
    stats = tools.Statistics(lambda ind: ind.fitness.values)
    stats.register("avg", np.mean)
    stats.register("min", np.min)
    
    print("\nStarting Genetic Algorithm optimization...")
    population, logbook = algorithms.eaSimple(
        population, toolbox, cxpb=cxpb, mutpb=mutpb,
        ngen=n_generations, stats=stats, halloffame=hof, verbose=True
    )
    
    best_ind = hof[0]
    print("\nBest individual is: ", best_ind, " with fitness: ", best_ind.fitness.values)
    
    nodes, hidden_layers, dropout_rate, batch_size, epochs, learning_rate = best_ind
    model = build_lstm_model(
        int(nodes), int(hidden_layers), dropout_rate, learning_rate, int(batch_size), int(epochs), 
        (X_train.shape[1], X_train.shape[2])
    )
    return train_and_evaluate_model(
        model, X_train, y_train, X_test, y_test, scaler,
        epochs=int(epochs), batch_size=int(batch_size), validation_split=0.2
    )

def main():
    data = load_data('path/to/scv/file') 
    X, y, scaler = prepare_data(data)
    split = int(0.8 * len(X))
    X_train, X_test = X[:split], X[split:]
    y_train, y_test = y[:split], y[split:]

    # 1. Grid Search
    print("\n=== Running Grid Search ===")
    gs_results = grid_search_simple_model(X_train, y_train, X_test, y_test, scaler)
    gs_model, gs_ypred_train, gs_ypred_test, gs_test_rmse, gs_test_mae, gs_history = gs_results

    # 2. Random Search
    print("\n=== Running Random Search ===")
    rs_results = random_search_simple_model(X_train, y_train, X_test, y_test, scaler)
    rs_model, rs_ypred_train, rs_ypred_test, rs_test_rmse, rs_test_mae, rs_history = rs_results

    # 3. Bayesian Optimization
    print("\n=== Running Bayesian Optimization ===")
    bo_results = bayesian_optimization_simple_model(X_train, y_train, X_test, y_test, scaler)
    bo_model, bo_ypred_train, bo_ypred_test, bo_test_rmse, bo_test_mae, bo_history = bo_results

    # 4. Genetic Algorithm
    print("\n=== Running Genetic Algorithm ===")
    ga_results = genetic_algorithm_optimization(X_train, y_train, X_test, y_test, scaler)
    ga_model, ga_ypred_train, ga_ypred_test, ga_test_rmse, ga_test_mae, ga_history = ga_results

    print("\n=== Model Comparison ===")
    print(f"{'Model':<25}{'RMSE':<15}{'MAE':<15}")
    print(f"{'Grid Search':<25}{gs_test_rmse:.4f}{'':<15}{gs_test_mae:.4f}")
    print(f"{'Random Search':<25}{rs_test_rmse:.4f}{'':<15}{rs_test_mae:.4f}")
    print(f"{'Bayesian Opt':<25}{bo_test_rmse:.4f}{'':<15}{bo_test_mae:.4f}")
    print(f"{'Genetic Algorithm':<25}{ga_test_rmse:.4f}{'':<15}{ga_test_mae:.4f}")

    plt.figure(figsize=(14, 5))

    actual_test = scaler.inverse_transform(np.concatenate((y_test.reshape(-1, 1), np.zeros((len(y_test), 1))), axis=1))[:, 0]
    test_dates = data.index[-len(y_test):]
    plt.plot(test_dates, actual_test, label='Actual Test Data', color='black')
    plt.plot(test_dates, gs_ypred_test, label='Grid search optimization', color='green', alpha=0.6)
    plt.plot(test_dates, rs_ypred_test, label='Random search optimization', color='blue', alpha=0.6)
    plt.plot(test_dates, bo_ypred_test, label='Bayesian optimization', color='purple', alpha=0.6)
    plt.plot(test_dates, ga_ypred_test, label='Genetic algorithm optimization', color='red', alpha=0.6)
    plt.title('Test Data Predictions')
    plt.ylabel('Price')
    plt.xticks(rotation=45)
    plt.legend()

    plt.tight_layout()
    plt.show()

if __name__ == "__main__":
    main()
